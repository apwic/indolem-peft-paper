\begin{abstract}
	Natural Language Processing (NLP) has become increasingly important in various applications, particularly in understanding and generating human language. The IndoLEM benchmark provides a comprehensive evaluation framework for Natural Language Understanding (NLU) tasks in the Indonesian language. Traditional fine-tuning methods, which involve updating all model parameters, can be computationally intensive and time-consuming. This study explores the application of Parameter-Efficient Fine-Tuning (PEFT) methods, specifically LoRA, Prefix-Tuning, Adapter, and UniPELT, to the IndoLEM tasks, including Named Entity Recognition (NER), Sentiment Analysis, and Summarization. By leveraging these techniques, the study aims to reduce the computational resources required for model training while maintaining competitive performance levels. The experimental results indicate that PEFT only uses approximately 0.2\% to 15\% of the modelâ€™s training parameters, with faster training times. The performance achieved for the NER and sentiment analysis tasks ranged from -0.8\% to -6.2\%, indicating a trade-off between the use of training parameters and the resulting performance. However, the Prefix-Tuning and UniPELT methods failed to provide consistent results on the summarization task. These findings contribute to the growing body of research on efficient NLP model training and provide practical insights for developing resource-efficient models in the Indonesian language context.
\end{abstract}

\begin{IEEEkeywords}
	Natural Language Processing, IndoLEM, Parameter-Efficient Fine-Tuning, Named Entity Recognition, Sentiment Analysis, Summarization, LoRA, Prefix-Tuning, Adapter, UniPELT
\end{IEEEkeywords}
