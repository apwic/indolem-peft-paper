\section{Related Works}

The evolution of fine-tuning methods in Natural Language Processing (NLP) has been significant, particularly with the introduction of Transformer-based models like BERT, GPT, and T5 \cite{bert, gpt, T5}. Traditional fine-tuning, which involves updating all model parameters during training, has been the dominant approach for adapting these large pre-trained models to specific tasks. While effective, this method is computationally intensive and requires substantial hardware resources, making it less accessible in resource-constrained environments. This limitation has sparked interest in more efficient methods, leading to the development of Parameter-Efficient Fine-Tuning (PEFT) techniques.

PEFT methods, such as Adapter, Low-Rank Adaptation (LoRA), and Prefix-Tuning, have been proposed to address the inefficiencies of traditional fine-tuning. Adapter methods, introduced by \citeauthor{adapter_houlsby} \cite{adapter_houlsby}, involve inserting small bottleneck layers (adapters) within each layer of the pre-trained model. These adapters are the only components updated during fine-tuning, significantly reducing the number of trainable parameters. The authors demonstrated that Adapter methods could achieve performance comparable to full fine-tuning while requiring much less computational power. This approach has been widely adopted and extended in various NLP tasks, proving particularly useful in multilingual and cross-lingual transfer learning scenarios \cite{adapters}.

LoRA, proposed by \citeauthor{lora} \cite{lora}, is another PEFT method that reduces the number of trainable parameters by decomposing the weight matrices of the model into low-rank approximations. LoRA enables efficient adaptation by modifying only a small fraction of the model's parameters, making it particularly suitable for scenarios where memory and computational resources are limited. The authors of LoRA demonstrated its effectiveness on several NLP benchmarks, showing that it could maintain performance levels close to full fine-tuning while being more resource-efficient.

Prefix-Tuning, introduced by \citeauthor{prefix_tuning} \cite{prefix_tuning}, is a method that prepends trainable prefix tokens to the input sequence of a Transformer model. Unlike traditional fine-tuning, which adjusts all model parameters, Prefix-Tuning only optimizes these prefix tokens, leaving the rest of the model parameters unchanged. This method has been shown to be highly effective in generative tasks, such as text summarization and dialogue generation, where it allows the model to adapt to specific tasks with minimal parameter updates. However, Prefix-Tuning's performance can vary depending on the complexity of the task, with some tasks benefiting more from this approach than others.

UniPELT, a more recent development by \citeauthor{unipelt} \cite{unipelt}, integrates multiple PEFT methods, including Adapter, LoRA, and Prefix-Tuning, into a unified framework. UniPELT dynamically selects and combines different PEFT methods during training, allowing it to adapt to the specific requirements of each task. This flexibility makes UniPELT particularly powerful in multitask learning scenarios, where different tasks may benefit from different fine-tuning strategies. The authors reported significant performance gains across a range of NLP benchmarks, highlighting the potential of UniPELT to generalize across tasks and domains.

In the context of Indonesian language processing, IndoLEM \cite{indolem} has emerged as a critical benchmark for evaluating NLP models' performance on Indonesian NLU tasks. Previous works on IndoLEM have primarily focused on applying traditional fine-tuning methods to pre-trained models like IndoBERT \cite{indolem}. While these approaches have yielded state-of-the-art results, they also highlight the computational challenges associated with fine-tuning large models on extensive datasets. The application of PEFT methods to IndoLEM tasks is a relatively unexplored area, offering the potential to improve efficiency without sacrificing performance.

