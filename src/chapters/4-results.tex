\section{Result \& Discussion}

This section presents and analyzes the results of applying various Parameter-Efficient Fine-Tuning (PEFT) methods—LoRA, Prefix Tuning, Adapter, and UniPELT—on the IndoLEM tasks, specifically Named Entity Recognition (NER), Sentiment Analysis, and Summarization. The discussion is organized into three main parts: model parameters, training time, and performance metrics.

\subsection{Model Parameters}

\begin{table}[!ht]
    \centering
    \caption{Model Parameters for IndoBERT and IndoT5}
    \label{table:param-model}
    \begin{tabular}{l|r|r}
        \toprule
        \textbf{Method} & \textbf{IndoBERT \#Parameter} & \textbf{IndoT5 \#Parameter} \\
        \midrule
        \textit{Fine-tuning} & \textbf{110M (100\%)} & \textbf{223M (100\%)} \\
        LoRA ($r$=8) & 0.294M (0.27\%) & 0.885M (0.40\%) \\
        LoRA ($r$=16) & 0.590M (0.54\%) & 1.769M (0.79\%) \\
        Prefix Tuning ($pl$=5) & 9.853M (8.91\%) & 29.560M (13.26\%) \\
        Prefix Tuning ($pl$=50) & 9.887M (8.94\%) & 29.663M (13.31\%) \\
        Adapter ($rf$=64) & \textcolor{Green}{0.231M (0.21\%)} & \textcolor{Green}{0.461M (0.21\%)} \\
        Adapter ($rf$=16) & 0.895M (0.81\%) & 1.789M (0.80\%) \\
        UniPELT & \textcolor{Red}{11.083M (10.03\%)} & \textcolor{Red}{32.346M (14.51\%)} \\
        \bottomrule
    \end{tabular}
\end{table}

The number of trainable parameters plays a crucial role in determining the efficiency and feasibility of deploying models in resource-constrained environments. Table \ref{table:param-model} shows the significant reduction in the number of parameters when using PEFT methods compared to traditional fine-tuning, which requires updating all model parameters.

LoRA, particularly with a rank of 8, emerges as the most parameter-efficient method, utilizing only 0.27\% of IndoBERT's parameters and 0.40\% of IndoT5's parameters. This drastic reduction indicates that LoRA can significantly decrease the computational resources required without severely impacting the model's capacity to adapt to the task. Similarly, Adapter with a reduction factor of 64 proves to be another highly efficient method, using only 0.21\% of the parameters in both IndoBERT and IndoT5. On the other hand, UniPELT, which combines multiple PEFT methods, has a considerably higher parameter count, particularly for IndoT5, where it requires 14.51\% of the parameters. This trade-off suggests that while UniPELT offers greater flexibility by combining various approaches, it also increases computational demands.

\subsection{Training Time}

\begin{table*}[!ht]
    \centering
    \caption{Average Training Runtime}
    \label{table:runtime}
    \begin{tabular}{l|r|r|r|r}
        \toprule
        \textbf{Method} & \textbf{NER UI (s)} & \textbf{NER UGM (s)} & \textbf{Sentiment (s)} & \textbf{Summarization (s)} \\
        \midrule
        \textit{Fine-tuning} & \textbf{884 (100\%)} & \textbf{974 (100\%)} & \textbf{866 (100\%)} & \textbf{5026 (100\%)} \\
        LoRA ($r$=8) & 528 (60\%) & 580 (60\%) & 627 (72\%) & 4841 (96\%) \\
        LoRA ($r$=16) & \textcolor{Green}{515 (58\%)} & 588 (60\%) & 627 (72\%) & 4900 (97\%) \\
        Prefix Tuning ($pl$=5) & 494 (56\%) & \textcolor{Green}{562 (58\%)} & \textcolor{Green}{619 (72\%)} & 6618 (132\%) \\
        Prefix Tuning ($pl$=50) & 534 (60\%) & 598 (61\%) & 652 (75\%) & \textcolor{Red}{11379 (227\%)} \\
        Adapter ($rf$=64) & 564 (64\%) & 587 (60\%) & {613 (71\%)} & \textcolor{Green}{4578 (91\%)} \\
        Adapter ($rf$=16) & 563 (64\%) & 595 (61\%) & 617 (71\%) & 4587 (91\%) \\
        UniPELT & \textcolor{Red}{928 (105\%)} & \textcolor{Red}{991 (102\%)} & \textcolor{Red}{746 (86\%)} & 9241 (184\%) \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[!ht]
    \centering
    \caption{Performance Result}
    \label{table:performance}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        \textbf{Method} & \textbf{NER UI (F1)} & \textbf{NER UGM (F1)} & \textbf{Sentiment (F1)} & \textbf{Summ (ROUGE-L)} \\
        \midrule
        \textit{Fine-tuning} & \textbf{0.952} & \textbf{0.859} & \textbf{0.876} & \textbf{65.525} \\
        LoRA ($r$=8) & {0.930} & 0.813 & 0.838 & \textcolor{Green}{66.538} \\
        LoRA ($r$=16) & \textcolor{Red}{0.930} & 0.818 & 0.856 & 66.507 \\
        Prefix Tuning ($pl$=5) & \textcolor{Green}{0.944} & \textcolor{Green}{0.839} & \textcolor{Green}{0.867} & 46.868 \\
        Prefix Tuning ($pl$=50) & 0.943 & 0.827 & 0.859 & 66.037 \\
        Adapter ($rf$=64) & 0.938 & 0.824 & 0.852 & 63.634 \\
        Adapter ($rf$=16) & 0.938 & 0.818 & \textcolor{Red}{0.821} & 56.735 \\
        UniPELT & 0.940 & \textcolor{Red}{0.784} & 0.862 & \textcolor{Red}{6.550} \\
        \bottomrule
    \end{tabular}
\end{table*}

Training time is a critical factor when considering the practical deployment of models, particularly in scenarios where time efficiency is paramount. Table \ref{table:runtime} summarizes the average training times for each method across the NER, Sentiment Analysis, and Summarization tasks.

For the NER tasks, LoRA and Prefix Tuning significantly reduced training times compared to the fine-tuning baseline. Specifically, LoRA with a rank of 16 achieved the lowest training time, completing the task in 58\% of the time required by fine-tuning. Similarly, Prefix Tuning with a prefix length of 5 reduced training time to 56\% and 58\% for the UI and UGM datasets, respectively. In contrast, UniPELT exhibited increased training times, surpassing the baseline fine-tuning time by approximately 5\% for UI and 2\% for UGM, likely due to the overhead associated with dynamically combining different PEFT strategies during training.

In the Sentiment Analysis task, all PEFT methods showed improved training efficiency over the baseline. The Adapter method with a reduction factor of 64 completed the task in just 71\% of the baseline time, making it the most time-efficient method. LoRA and Prefix Tuning also demonstrated efficient performance, with training times ranging from 72\% to 75\% of the baseline. However, UniPELT, while still faster than fine-tuning, required 86\% of the baseline training time, reflecting the complexity of its multi-method approach.

For the Summarization task, Adapter with a reduction factor of 64 again demonstrated superior efficiency, reducing training time to 91\% of the baseline. LoRA with ranks of 8 and 16 showed slightly higher training times, at 96\% and 97\% of the baseline, respectively. Notably, Prefix Tuning with a prefix length of 50 and UniPELT both exhibited significantly increased training times, with Prefix Tuning reaching 227\% of the baseline and UniPELT 184\%. This substantial increase highlights the limitations of these methods in tasks requiring extensive sequence generation.

\subsection{Performance Metrics}

Performance metrics, including F1 scores for NER and Sentiment Analysis and ROUGE scores for Summarization, provide a direct measure of how well each method performs relative to the baseline. Table \ref{table:performance} presents a summary of these metrics. In terms of performance, fine-tuning generally provided the best results across all tasks, serving as the baseline. For the NER task, Prefix Tuning with a prefix length of 5 achieved the highest F1 score on the UI dataset (0.944), closely matching the baseline performance, while LoRA and Adapter methods also performed well. However, for the UGM dataset, all PEFT methods exhibited lower F1 scores compared to the baseline, with UniPELT showing the greatest drop to 0.784.

For Sentiment Analysis, the fine-tuning baseline outperformed all PEFT methods with an F1 score of 0.876. Among the PEFT methods, Prefix Tuning with a prefix length of 5 again achieved the highest score (0.867), followed by UniPELT with 0.862, indicating that these methods can still achieve competitive performance, albeit with a slight trade-off.

In the Summarization task, the ROUGE scores reflect the ability of the models to generate summaries that closely match the reference texts. Here, LoRA with a rank of 8 and 16 slightly outperformed the baseline in terms of ROUGE-1 and ROUGE-L scores, indicating that LoRA is particularly effective for sequence generation tasks. Conversely, Prefix Tuning with a prefix length of 5 and UniPELT performed poorly, with UniPELT's ROUGE scores dropping significantly, suggesting that these methods are less suited to tasks requiring extensive text generation.

\subsection{Discussion}

The results demonstrate that Parameter-Efficient Fine-Tuning (PEFT) methods can provide competitive performance with reduced computational needs, though effectiveness varies by task.

\subsubsection{\textbf{Parameter Efficiency vs. Model Performance}}
LoRA, especially with a rank of 8, offers a significant reduction in trainable parameters while maintaining strong performance, making it ideal for resource-constrained settings. However, tasks requiring more context, like Summarization, show diminishing returns for methods like UniPELT and long-prefix Prefix-Tuning, suggesting these methods may introduce overhead that affects performance on longer sequences.

\subsubsection{\textbf{Training Time vs. Efficiency}}
LoRA and Adapter consistently reduce training time across tasks, making them ideal for scenarios requiring frequent retraining. However, Prefix-Tuning with longer prefixes and UniPELT increased training time, making them less suitable for time-sensitive applications. The balance between reduced parameters and training time should guide method selection based on application needs.

\subsubsection{\textbf{Task-Specific Considerations}}
The performance of PEFT methods varies across different tasks, making it crucial to choose the right method based on the task requirements. Key insights include:

\begin{itemize}
    \item \textbf{Named Entity Recognition (NER):} LoRA and Adapter provided the best balance of efficiency and performance, making them ideal for NER tasks. These methods significantly reduced training time while maintaining high F1 scores, making them suitable for resource-constrained environments where quick retraining is essential.
    \item \textbf{Sentiment Analysis:} Similar to NER, both LoRA and Adapter performed well in Sentiment Analysis tasks, with reduced parameter requirements and training times. Their ability to maintain strong accuracy and F1 scores while reducing computational overhead makes them suitable for tasks requiring frequent updates.
    \item \textbf{Summarization:} Summarization tasks, which involve longer sequences and more complex context generation, presented more challenges for PEFT methods. While LoRA and Adapter showed moderate effectiveness, longer-prefix Prefix-Tuning and UniPELT struggled with performance degradation and increased training time, suggesting these methods may not be suitable for tasks requiring extensive text generation.
\end{itemize}

\subsubsection{\textbf{Limitations}}
While LoRA and Adapter performed well for classification tasks like NER and Sentiment Analysis, their effectiveness in more complex generation tasks, such as Summarization, was limited. Future research should focus on hybrid methods that combine the strengths of multiple PEFT techniques while addressing their weaknesses. Additionally, there is room for optimizing methods like UniPELT to reduce training overhead without sacrificing performance.

Furthermore, expanding the use of PEFT methods to low-resource languages, particularly languages like Indonesian, offers an exciting research opportunity. The unique linguistic features of these languages may impact how PEFT methods perform, and more studies should be conducted to generalize the findings across different languages and linguistic structures.

