\section{Conclusion \& Future Works}

This study explored the application of Parameter-Efficient Fine-Tuning (PEFT) methods—LoRA, Prefix Tuning, Adapter, and UniPELT—on Indonesian NLP tasks, including Named Entity Recognition (NER), Sentiment Analysis, and Summarization. The results demonstrated that PEFT methods like LoRA and Adapter could significantly reduce the number of trainable parameters and improve training efficiency while maintaining competitive performance. For instance, LoRA with a rank of 8 reduced the parameter count to just 0.27\% of the full model, achieving near-baseline performance in many tasks. Adapter methods similarly offered a good balance between efficiency and performance, especially in Sentiment Analysis and Summarization.

However, the study also highlighted some limitations of PEFT methods. For example, while Prefix Tuning showed strong performance in NER tasks with reduced training time, its effectiveness varied across different datasets. UniPELT, which combines multiple PEFT strategies, demonstrated increased training times and did not consistently outperform simpler methods, particularly in tasks like Summarization. These findings suggest that while PEFT methods are promising, their effectiveness is task-dependent, and choosing the right method requires careful consideration of the specific task and dataset characteristics.

Looking forward, several research directions could further enhance the understanding and application of PEFT methods. Future work could explore other emerging PEFT techniques, optimize methods for specific tasks, and extend the research to other languages and domains. Additionally, integrating PEFT methods with other efficiency techniques like model pruning or quantization could yield even more lightweight models. Real-world deployment and user-centric evaluations would also be valuable to assess the practical impact of these methods and refine them based on feedback from actual users.

In conclusion, while PEFT methods like LoRA and Adapter offer substantial benefits in terms of efficiency, their application must be carefully tailored to the specific needs of the task. Further research and development will help to refine these methods, making sophisticated NLP models more accessible and effective in resource-constrained environments. By pursuing these avenues, we can continue to advance the field of NLP, particularly for low-resource languages like Indonesian, where computational efficiency is crucial.
