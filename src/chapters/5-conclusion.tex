\section{Conclusion \& Future Works}

The experimental results indicate that PEFT methods can achieve comparable performance to traditional fine-tuning with significantly fewer parameters. In particular, the results show that PEFT methods only require approximately 0.2\% to 15\% of the modelâ€™s training parameters, leading to faster training times. However, this efficiency comes with trade-offs, as observed in the performance variations across different tasks. For instance, while NER and Sentiment Analysis tasks exhibited minimal performance degradation, ranging from -0.8\% to -6.2\%, the Summarization task presented more challenges. The Prefix-Tuning and UniPELT methods, in particular, struggled to produce consistent results in Summarization, highlighting the limitations of these methods in handling certain types of NLP tasks.

Looking forward, several research directions could further enhance the understanding and application of PEFT methods. Future work could explore other emerging PEFT techniques, optimize methods for specific tasks, and extend the research to other languages and domains. Additionally, integrating PEFT methods with other efficiency techniques like model pruning or quantization could yield even more lightweight models. Real-world deployment and user-centric evaluations would also be valuable to assess the practical impact of these methods and refine them based on feedback from actual users.

