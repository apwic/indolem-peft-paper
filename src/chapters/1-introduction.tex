\section{Introduction}

The rapid development of Natural Language Processing (NLP) has led to significant advancements in the way machines understand and interact with human language. NLP has become an essential component in a wide range of applications, from machine translation and sentiment analysis to automated summarization and conversational agents. The ability to process and generate human language efficiently and accurately is critical in domains such as information retrieval, social media analysis, and customer service automation. However, these advancements come with challenges, particularly in terms of computational efficiency and resource requirements \cite{nlp, ai}.

One of the most prominent benchmarks for evaluating the performance of NLP models in the Indonesian language is the IndoLEM (Indonesian Language Evaluation Models) \cite{indolem}. IndoLEM provides a comprehensive evaluation framework for various Natural Language Understanding (NLU) tasks, including Named Entity Recognition (NER), Sentiment Analysis, and Summarization. These tasks are fundamental in NLP as they enable machines to identify entities within text, assess the sentiment behind expressions, and generate concise summaries of larger bodies of text. However, traditional methods of fine-tuning NLP models, which involve updating all the model parameters, are often computationally expensive and time-consuming. This has led to the exploration of alternative methods that can achieve similar levels of performance with greater efficiency.

Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a promising solution to the challenges posed by traditional fine-tuning techniques. PEFT methods, such as Low-Rank Adaptation (LoRA), Prefix-Tuning, Adapter, and UniPELT, offer a means to fine-tune models with a significantly reduced number of trainable parameters \cite{adapter_houlsby, prefix_tuning, lora, unipelt}. By doing so, these methods aim to lower the computational cost and time required for model training, making it feasible to deploy advanced NLP models in resource-constrained environments. Moreover, PEFT methods retain the core knowledge of pre-trained models while adapting them to specific tasks, thus striking a balance between efficiency and effectiveness.

The application of PEFT methods to the IndoLEM tasks presents an opportunity to evaluate their performance in a linguistically and culturally rich context. Indonesian, being a highly agglutinative language with diverse dialects and regional variations, poses unique challenges for NLP models. The IndoLEM benchmark provides a rigorous testbed for assessing the capabilities of PEFT methods in handling such linguistic complexities. This study focuses on applying LoRA, Prefix-Tuning, Adapter, and UniPELT methods to IndoLEM tasks, comparing their performance to traditional fine-tuning approaches, and analyzing their effectiveness in terms of computational efficiency and model accuracy.

In this research, the objective is to explore the trade-offs between efficiency and performance when applying PEFT methods to NLU tasks in Indonesian. Specifically, the study examines the extent to which PEFT methods can reduce the number of trainable parameters while maintaining or even enhancing the model's ability to perform tasks such as NER, Sentiment Analysis, and Summarization. Furthermore, the study aims to identify the scenarios in which these methods excel and where they may fall short, providing insights into their practical applicability in real-world settings.

The experimental results indicate that PEFT methods can achieve comparable performance to traditional fine-tuning with significantly fewer parameters. In particular, the results show that PEFT methods only require approximately 0.2\% to 15\% of the model's training parameters, leading to faster training times. However, this efficiency comes with trade-offs, as observed in the performance variations across different tasks. For instance, while NER and Sentiment Analysis tasks exhibited minimal performance degradation, ranging from -0.8\% to -6.2\%, the Summarization task presented more challenges. The Prefix-Tuning and UniPELT methods, in particular, struggled to produce consistent results in Summarization, highlighting the limitations of these methods in handling certain types of NLP tasks.

This paper is structured as follows: Section II reviews related work on PEFT methods and their application in NLP. Section III details the methodology used in this study, including the experimental setup and evaluation metrics. Section IV presents the experimental results and discusses the trade-offs between efficiency and performance. Finally, Section V concludes the paper with a summary of the findings and suggestions for future research.
