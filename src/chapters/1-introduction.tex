\section{Introduction}

The rapid development of Natural Language Processing (NLP) has led to significant advancements in the way machines understand and interact with human language. NLP has become an essential component in a wide range of applications, from machine translation and sentiment analysis to automated summarization and conversational agents. The ability to process and generate human language efficiently and accurately is critical in domains such as information retrieval, social media analysis, and customer service automation. However, these advancements come with challenges, particularly in terms of computational efficiency and resource requirements \cite{nlp, ai}.

One of the most prominent benchmarks for evaluating the performance of NLP models in the Indonesian language is the IndoLEM (Indonesian Language Evaluation Models) \cite{indolem}. IndoLEM provides a comprehensive evaluation framework for various Natural Language Understanding (NLU) tasks, including Named Entity Recognition (NER), Sentiment Analysis, and Summarization. These tasks are fundamental in NLP as they enable machines to identify entities within text, assess the sentiment behind expressions, and generate concise summaries of larger bodies of text. However, traditional methods of fine-tuning NLP models, which involve updating all the model parameters, are often computationally expensive and time-consuming. This has led to the exploration of alternative methods that can achieve similar levels of performance with greater efficiency.

Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a promising solution to the challenges posed by traditional fine-tuning techniques. PEFT methods, such as Low-Rank Adaptation (LoRA), Prefix-Tuning, Adapter, and UniPELT, offer a means to fine-tune models with a significantly reduced number of trainable parameters \cite{adapter_houlsby, prefix_tuning, lora, unipelt}. By doing so, these methods aim to lower the computational cost and time required for model training, making it feasible to deploy advanced NLP models in resource-constrained environments. Moreover, PEFT methods retain the core knowledge of pre-trained models while adapting them to specific tasks, thus striking a balance between efficiency and effectiveness.

Although research comparing PEFT methods has been conducted in widely spoken languages like English, there has been limited exploration of their application to the Indonesian language. This absence is particularly significant because Indonesian presents unique linguistic challenges that may affect the performance and efficiency of NLP models. Moreover, there is a growing demand for low-resource language solutions, where the efficiency gains offered by PEFT methods could be especially beneficial. This study aims to fill this gap by applying and evaluating PEFT methods on IndoLEM, providing insights into their applicability and performance for Indonesian NLU tasks.

The application of PEFT methods to the IndoLEM tasks presents an opportunity to evaluate their performance in a linguistically and culturally rich context. The IndoLEM benchmark provides a rigorous testbed for assessing the capabilities of PEFT methods in handling such linguistic complexities. This study focuses on applying LoRA, Prefix-Tuning, Adapter, and UniPELT methods to IndoLEM tasks, comparing their performance to traditional fine-tuning approaches, and analyzing their effectiveness in terms of computational efficiency and model performance.

This study builds upon the existing research on PEFT methods by applying them to IndoLEM tasks and evaluating their effectiveness compared to traditional fine-tuning. By leveraging methods such as Adapter, LoRA, Prefix-Tuning, and UniPELT, this research aims to address the computational challenges of fine-tuning in resource-constrained environments, particularly in the context of Indonesian language processing. The results of this study contribute to the broader understanding of how PEFT methods can be effectively utilized in various linguistic and cultural contexts, paving the way for more efficient NLP model deployment in real-world applications.

In this research, the objective is to explore the trade-offs between efficiency and performance when applying PEFT methods to NLU tasks in Indonesian. Specifically, the study examines the extent to which PEFT methods can reduce the number of trainable parameters while maintaining or even enhancing the model's ability to perform tasks such as NER, Sentiment Analysis, and Summarization. Furthermore, the study aims to identify the scenarios in which these methods excel and where they may fall short, providing insights into their practical applicability in real-world settings.

This paper is structured as follows: Section II reviews related work on PEFT methods and their application in NLP. Section III details the methodology used in this study, including the experimental setup and evaluation metrics. Section IV presents the experimental results and discusses the trade-offs between efficiency and performance. Finally, Section V concludes the paper with a summary of the findings and suggestions for future research.
